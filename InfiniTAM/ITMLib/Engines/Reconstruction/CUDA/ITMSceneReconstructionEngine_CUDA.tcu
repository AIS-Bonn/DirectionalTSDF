// Copyright 2014-2017 Oxford University Innovation Limited and the authors of InfiniTAM

#pragma once

#include <memory>
#include <ITMLib/Utils/ITMLibSettings.h>
#include <ITMLib/Utils/ITMSceneParams.h>
#include <ITMLib/Utils/ITMTimer.h>
#include <ITMLib/Objects/Scene/TSDF_CUDA.h>
#include "ITMSceneReconstructionEngine_CUDA.h"
#include "ITMSummingVoxelMap_CUDA.h"

#include "../Shared/ITMSceneReconstructionEngine_Shared.h"
#include "ITMLib/Objects/RenderStates/ITMRenderState_VH.h"
#include "ITMLib/Utils/ITMCUDAUtils.h"
#include <stdgpu/memory.h>
#include <stdgpu/unordered_map.cuh>
#include <stdgpu/unordered_set.cuh>

using namespace ITMLib;

namespace
{
// device functions

void
__global__
rayCastClearSum_device(
	const ITMVoxelBlockHash::IndexData *hashTable,
	const int *visibleEntryIds,
	SummingVoxel *entriesRayCasting
)
{
	int x = threadIdx.x, y = threadIdx.y, z = threadIdx.z;
	int locId = x + y * SDF_BLOCK_SIZE + z * SDF_BLOCK_SIZE * SDF_BLOCK_SIZE;
	int entryId = blockIdx.x;

	const ITMHashEntry &hashEntry = hashTable[visibleEntryIds[entryId]];
	if (not hashEntry.IsValid())
		return;

	entriesRayCasting[hashEntry.ptr * SDF_BLOCK_SIZE3 + locId].reset();
}


__global__
void rayCastCombine_device(
	ITMVoxel *localVBA,
	const ITMVoxelBlockHash::IndexData *hashTable,
	const int *visibleEntryIds,
	const SummingVoxel *summingVoxels,
	const ITMSceneParams sceneParams
)
{
	int x = threadIdx.x, y = threadIdx.y, z = threadIdx.z;
	int locId = VoxelIndicesToOffset(x, y, z);
	size_t entryId = blockIdx.x;

	const ITMHashEntry &hashEntry = hashTable[visibleEntryIds[entryId]];
	if (not hashEntry.IsValid())
		return;

	ITMVoxel *localVoxelBlock = &(localVBA[hashEntry.ptr * SDF_BLOCK_SIZE3]);

	// assumption: summingVoxels added in visibleEntryIds order (valid for ITMSummingVoxelMap_CUDA)
	const SummingVoxel *rayCastingSum = &(summingVoxels[entryId * SDF_BLOCK_SIZE3]);

	rayCastCombine(localVoxelBlock[locId], rayCastingSum[locId], sceneParams);
}

__global__
void rayCastUpdate_device(
	Vector2i imgSize_d, Vector2i imgSize_rgb, float* depth, Vector4f* depthNormals, Vector4u* rgb,
	const Matrix4f invM_d, const Matrix4f invM_rgb,
	const Vector4f invProjParams_d, const Vector4f invProjParams_rgb,
	const ITMFusionParams fusionParams,
	const ITMSceneParams sceneParams,
	stdgpu::unordered_map<IndexType, SummingVoxel*> summingVoxelMap
	)
{
	int x = threadIdx.x + blockIdx.x * blockDim.x;
	int y = threadIdx.y + blockIdx.y * blockDim.y;

	if (x >= imgSize_d.x or y >= imgSize_d.y)
		return;

	rayCastUpdate(x, y, imgSize_d, imgSize_rgb, depth, depthNormals, rgb, invM_d, invM_rgb,
	              invProjParams_d, invProjParams_rgb, fusionParams, sceneParams, summingVoxelMap);
}

__global__
void rayCastCarveSpace_device(
	Vector2i imgSize, float* depth, Vector4f* depthNormals,
	const Matrix4f invM_d,
	const Vector4f invProjParams_d, const Vector4f invProjParams_rgb,
	const ITMFusionParams fusionParams,
	const ITMSceneParams sceneParams,
	const ITMHashEntry* hashTable,
	stdgpu::unordered_map<IndexType, SummingVoxel*> summingVoxelMap,
	ITMVoxel *localVBA)
{
	int x = threadIdx.x + blockIdx.x * blockDim.x;
	int y = threadIdx.y + blockIdx.y * blockDim.y;

	if (x >= imgSize.x or y >= imgSize.y)
		return;

	rayCastCarveSpace(x, y, imgSize, depth, depthNormals, invM_d,
	                  invProjParams_d, invProjParams_rgb, fusionParams, sceneParams,
	                  hashTable, summingVoxelMap, localVBA);
}

template<bool stopMaxW>
__global__ void voxelProjectionCarveSpace_device(
	ITMVoxel *localVBA, SummingVoxel* summingVoxels,
	const ITMHashEntry *hashTable, int *visibleEntryIDs,
	const Vector4u *rgb, Vector2i rgbImgSize, const float *depth, const Vector4f *depthNormals,
	const float *confidence, Vector2i depthImgSize, Matrix4f M_d, Matrix4f M_rgb, Vector4f projParams_d,
	Vector4f projParams_rgb, const ITMFusionParams fusionParams,
	const ITMSceneParams sceneParams)
{
	Vector3i globalPos;
	int entryId = blockIdx.x;

	const ITMHashEntry &currentHashEntry = hashTable[visibleEntryIDs[entryId]];

	if (currentHashEntry.ptr < 0) return;

	globalPos = currentHashEntry.pos.toInt() * SDF_BLOCK_SIZE;

	ITMVoxel *localVoxelBlock = &(localVBA[currentHashEntry.ptr * SDF_BLOCK_SIZE3]);

	// assumption: summingVoxels added in visibleEntryIds order (valid for ITMSummingVoxelMap_CUDA)
	SummingVoxel *localRayCastingSum = &(summingVoxels[entryId * SDF_BLOCK_SIZE3]);

	int x = threadIdx.x, y = threadIdx.y, z = threadIdx.z;

	Vector4f pt_model; int locId;

	locId = x + y * SDF_BLOCK_SIZE + z * SDF_BLOCK_SIZE * SDF_BLOCK_SIZE;

	if (stopMaxW) if (localVoxelBlock[locId].w_depth == sceneParams.maxW) return;
	//if (approximateIntegration) if (localVoxelBlock[locId].w_depth != 0) return;

	float voxelSize = sceneParams.voxelSize;
	pt_model.x = (float)(globalPos.x + x) * voxelSize;
	pt_model.y = (float)(globalPos.y + y) * voxelSize;
	pt_model.z = (float)(globalPos.z + z) * voxelSize;
	pt_model.w = 1.0f;

	voxelProjectionCarveSpace(
		localVoxelBlock[locId], localRayCastingSum[locId],
		TSDFDirection(currentHashEntry.direction),
		pt_model, M_d, projParams_d, M_rgb, projParams_rgb,
		fusionParams, sceneParams, depth, depthNormals, confidence,
		depthImgSize, rgb, rgbImgSize);
}

template<bool stopMaxW>
__global__ void integrateIntoScene_device(stdgpu::unordered_map<ITMIndexDirectional, ITMVoxel*> tsdf, ITMIndexDirectional* visibleEntries,
                                          const Vector4u* rgb, Vector2i rgbImgSize, const float* depth,
                                          const Vector4f* depthNormals, const float* confidence, Vector2i depthImgSize,
                                          Matrix4f M_d, Matrix4f M_rgb, Vector4f projParams_d, Vector4f projParams_rgb,
                                          float _voxelSize, const ITMFusionParams fusionParams,
                                          const ITMSceneParams sceneParams)
{

	const ITMIndexDirectional& index = visibleEntries[blockIdx.x];
	auto it = tsdf.find(index);
	if (it == tsdf.end())
		return;
	ITMVoxel *localVoxelBlock = it->second;

	int x = threadIdx.x, y = threadIdx.y, z = threadIdx.z;

	Vector4f pt_model; int locId;

	locId = x + y * SDF_BLOCK_SIZE + z * SDF_BLOCK_SIZE * SDF_BLOCK_SIZE;

	if (stopMaxW) if (localVoxelBlock[locId].w_depth == sceneParams.maxW) return;

	pt_model = Vector4f(voxelIdxToWorldPos(index.getPosition() * SDF_BLOCK_SIZE + Vector3i(x, y, z), _voxelSize), 1.0f);

	std::conditional<ITMVoxel::hasColorInformation, ComputeUpdatedVoxelInfo<true, ITMVoxel>, ComputeUpdatedVoxelInfo<false, ITMVoxel>>::type::compute(
		localVoxelBlock[locId], index.getDirection(),
		pt_model, M_d, projParams_d, M_rgb, projParams_rgb, fusionParams, sceneParams, depth, depthNormals, confidence,
		depthImgSize, rgb, rgbImgSize);
}

template<bool stopMaxW>
__global__ void integrateIntoScene_device(ITMVoxel *localVBA, const ITMHashEntry *hashTable, int *visibleEntryIDs,
	const Vector4u *rgb, Vector2i rgbImgSize, const float *depth, const Vector4f *depthNormals, const float *confidence, Vector2i depthImgSize, Matrix4f M_d, Matrix4f M_rgb, Vector4f projParams_d,
	Vector4f projParams_rgb, float _voxelSize, const ITMFusionParams fusionParams, const ITMSceneParams sceneParams)
{
	Vector3i globalPos;
	int entryId = visibleEntryIDs[blockIdx.x];

	const ITMHashEntry &currentHashEntry = hashTable[entryId];

	if (currentHashEntry.ptr < 0) return;

	globalPos = blockToVoxelPos(currentHashEntry.pos.toInt());

	ITMVoxel *localVoxelBlock = &(localVBA[currentHashEntry.ptr * SDF_BLOCK_SIZE3]);

	int x = threadIdx.x, y = threadIdx.y, z = threadIdx.z;

	Vector4f pt_model; int locId;

	locId = x + y * SDF_BLOCK_SIZE + z * SDF_BLOCK_SIZE * SDF_BLOCK_SIZE;

	if (stopMaxW) if (localVoxelBlock[locId].w_depth == sceneParams.maxW) return;
	//if (approximateIntegration) if (localVoxelBlock[locId].w_depth != 0) return;

	pt_model = Vector4f(voxelIdxToWorldPos(globalPos + Vector3i(x, y, z), _voxelSize), 1.0f);

		std::conditional<ITMVoxel::hasColorInformation, ComputeUpdatedVoxelInfo<true, ITMVoxel>, ComputeUpdatedVoxelInfo<false, ITMVoxel>>::type::compute(
			localVoxelBlock[locId], TSDFDirection(currentHashEntry.direction),
		pt_model, M_d, projParams_d, M_rgb, projParams_rgb, fusionParams, sceneParams, depth, depthNormals, confidence,
		depthImgSize, rgb, rgbImgSize);
}

template<template<typename...> class Set, typename... Args>
__global__ void findAllocationBlocks_device(Set<ITMIndexDirectional, Args...> visibleBlocks,
                                                    const float* depth, const Vector4f* depthNormal,
                                                    Matrix4f invM_d, Vector4f projParams_d, float mu, Vector2i imgSize, float voxelSize,
                                                    float viewFrustum_min, float viewFrustum_max, const ITMFusionParams fusionParams)
{
	int x = threadIdx.x + blockIdx.x * blockDim.x, y = threadIdx.y + blockIdx.y * blockDim.y;
	if (x > imgSize.x - 1 || y > imgSize.y - 1) return;

	findAllocationBlocks<Set, Args...>(visibleBlocks, x, y, depth, depthNormal, invM_d, projParams_d, mu, imgSize, voxelSize,
	                        viewFrustum_min, viewFrustum_max, fusionParams);
}

__global__ void buildHashAllocAndVisibleType_device(HashEntryAllocType *entriesAllocType, HashEntryVisibilityType *entriesVisibleType,
	                                                  Vector4s *blockCoords, TSDFDirection *blockDirections, const float *depth,
	                                                  const Vector4f *depthNormal, Matrix4f invM_d, Vector4f projParams_d, float mu, Vector2i _imgSize,
	                                                  float _voxelSize, ITMHashEntry *hashTable, float viewFrustum_min, float viewFrustum_max,
	                                                  const ITMFusionParams fusionParams)
{
	int x = threadIdx.x + blockIdx.x * blockDim.x, y = threadIdx.y + blockIdx.y * blockDim.y;

	if (x > _imgSize.x - 1 || y > _imgSize.y - 1) return;

	if (fusionParams.useSpaceCarving)
		buildSpaceCarvingVisibleType(entriesVisibleType, x, y, blockCoords, blockDirections,
		                             depth, depthNormal, invM_d, projParams_d, mu, _imgSize, _voxelSize, hashTable,
		                             viewFrustum_min, viewFrustum_max, fusionParams);

	buildHashAllocAndVisibleType(entriesAllocType, entriesVisibleType, x, y, blockCoords, blockDirections,
		depth, depthNormal, invM_d, projParams_d, mu, _imgSize, _voxelSize, hashTable, viewFrustum_min,
		viewFrustum_max, fusionParams);
}

__global__ void setToType3(HashEntryVisibilityType *entriesVisibleType, int *visibleEntryIDs, int noVisibleEntries)
{
	int entryId = threadIdx.x + blockIdx.x * blockDim.x;
	if (entryId > noVisibleEntries - 1) return;
	entriesVisibleType[visibleEntryIDs[entryId]] = PREVIOUSLY_VISIBLE;
}

__global__ void allocateVoxelBlocksList_device(int *voxelAllocationList, int *excessAllocationList, ITMHashEntry *hashTable, int noTotalEntries,
	AllocationTempData *allocData, HashEntryAllocType *entriesAllocType, HashEntryVisibilityType *entriesVisibleType,
	Vector4s *blockCoords, TSDFDirection *blockDirections)
{
	int targetIdx = threadIdx.x + blockIdx.x * blockDim.x;
	if (targetIdx > noTotalEntries - 1) return;

	int vbaIdx, exlIdx;

	switch (entriesAllocType[targetIdx])
	{
	case ALLOCATE_ORDERED: //needs allocation, fits in the ordered list
		vbaIdx = atomicSub(&allocData->noAllocatedVoxelEntries, 1);

		if (vbaIdx >= 0) //there is room in the voxel block array
		{
			Vector4s pt_block_all = blockCoords[targetIdx];

			ITMHashEntry hashEntry;
			hashEntry.pos.x = pt_block_all.x; hashEntry.pos.y = pt_block_all.y; hashEntry.pos.z = pt_block_all.z;
			hashEntry.ptr = voxelAllocationList[vbaIdx];
			hashEntry.offset = 0;
			hashEntry.direction = static_cast<TSDFDirection_type>(blockDirections[targetIdx]);

			hashTable[targetIdx] = hashEntry;

			atomicAdd(&allocData->noAllocationsPerDirection[hashEntry.direction % 255], 1);
		}
		else
		{
			// Mark entry as not visible since we couldn't allocate it but buildHashAllocAndVisibleType changed its state.
			entriesVisibleType[targetIdx] = INVISIBLE;

			// Restore the previous value to avoid leaks.
			atomicAdd(&allocData->noAllocatedVoxelEntries, 1);
		}
		break;

	case ALLOCATE_EXCESS: //needs allocation in the excess list
		vbaIdx = atomicSub(&allocData->noAllocatedVoxelEntries, 1);
		exlIdx = atomicSub(&allocData->noAllocatedExcessEntries, 1);

		if (vbaIdx >= 0 && exlIdx >= 0) //there is room in the voxel block array and excess list
		{
			Vector4s pt_block_all = blockCoords[targetIdx];

			ITMHashEntry hashEntry;
			hashEntry.pos.x = pt_block_all.x; hashEntry.pos.y = pt_block_all.y; hashEntry.pos.z = pt_block_all.z;
			hashEntry.ptr = voxelAllocationList[vbaIdx];
			hashEntry.offset = 0;
			hashEntry.direction = static_cast<TSDFDirection_type>(blockDirections[targetIdx]);

			int exlOffset = excessAllocationList[exlIdx];

			hashTable[targetIdx].offset = exlOffset + 1; //connect to child

			hashTable[SDF_BUCKET_NUM + exlOffset] = hashEntry; //add child to the excess list

			entriesVisibleType[SDF_BUCKET_NUM + exlOffset] = VISIBLE_IN_MEMORY; //make child visible

			atomicAdd(&allocData->noAllocationsPerDirection[hashEntry.direction % 255], 1);
		}
		else
		{
			// No need to mark the entry as not visible since buildHashAllocAndVisibleType did not mark it.
			// Restore the previous values to avoid leaks.
			atomicAdd(&allocData->noAllocatedVoxelEntries, 1);
			atomicAdd(&allocData->noAllocatedExcessEntries, 1);
		}

		break;
	}
}

__global__ void reAllocateSwappedOutVoxelBlocks_device(int *voxelAllocationList, ITMHashEntry *hashTable, int noTotalEntries,
	AllocationTempData *allocData, /*int *noAllocatedVoxelEntries,*/ HashEntryVisibilityType *entriesVisibleType)
{
	int targetIdx = threadIdx.x + blockIdx.x * blockDim.x;
	if (targetIdx > noTotalEntries - 1) return;

	int vbaIdx;
	int hashEntry_ptr = hashTable[targetIdx].ptr;

	if (entriesVisibleType[targetIdx] > 0 && hashEntry_ptr == -1) //it is visible and has been previously allocated inside the hash, but deallocated from VBA
	{
		vbaIdx = atomicSub(&allocData->noAllocatedVoxelEntries, 1);
		if (vbaIdx >= 0) hashTable[targetIdx].ptr = voxelAllocationList[vbaIdx];
		else atomicAdd(&allocData->noAllocatedVoxelEntries, 1);
	}
}

template<bool useSwapping>
__global__ void buildVisibleList_kernel(ITMHashEntry* hashTable, ITMHashSwapState* swapStates, int noTotalEntries,
                                        int* visibleEntryIDs, AllocationTempData* allocData,
                                        HashEntryVisibilityType* entriesVisibleType,
                                        Matrix4f M_d, Vector4f projParams_d, Vector2i depthImgSize, float voxelSize)
{
	int targetIdx = threadIdx.x + blockIdx.x * blockDim.x;
	if (targetIdx > noTotalEntries - 1) return;

	__shared__ bool shouldPrefix;
	shouldPrefix = false;
	__syncthreads();

	buildVisibleList<useSwapping>(hashTable, swapStates, noTotalEntries, visibleEntryIDs,
	                              allocData, entriesVisibleType, M_d, projParams_d, depthImgSize, voxelSize,
	                              targetIdx);

	HashEntryVisibilityType hashVisibleType = entriesVisibleType[targetIdx];
	if (hashVisibleType > 0) shouldPrefix = true;

	__syncthreads();

	if (shouldPrefix)
	{
		int offset = computePrefixSum_device<int>(hashVisibleType > 0, &allocData->noVisibleEntries, blockDim.x * blockDim.y, threadIdx.x);
		if (offset != -1) visibleEntryIDs[offset] = targetIdx;
	}

#if 0
	// "active list": blocks that have new information from depth image
	// currently not used...
	__syncthreads();

	if (shouldPrefix)
	{
		int offset = computePrefixSum_device<int>(hashVisibleType == 1, noActiveEntries, blockDim.x * blockDim.y, threadIdx.x);
		if (offset != -1) activeEntryIDs[offset] = targetIdx;
	}
#endif
}

}

ITMSceneReconstructionEngine_CUDA::ITMSceneReconstructionEngine_CUDA(
	const std::shared_ptr<const ITMLibSettings>& settings)
	: ITMSceneReconstructionEngine(settings)
{
	ORcudaSafeCall(cudaMalloc((void**) &allocationTempData_device, sizeof(AllocationTempData)));
	ORcudaSafeCall(cudaMallocHost((void**) &allocationTempData_host, sizeof(AllocationTempData)));

	allocationBlocksList = new ORUtils::MemoryBlock<ITMIndexDirectional>(1, MEMORYDEVICE_CUDA);

	int noTotalEntries = ITMVoxelBlockHash::noTotalEntries;
	ORcudaSafeCall(cudaMalloc((void**) &entriesAllocType_device, noTotalEntries * sizeof(HashEntryAllocType)));
	ORcudaSafeCall(cudaMalloc((void**) &blockCoords_device, noTotalEntries * sizeof(Vector4s)));
	ORcudaSafeCall(cudaMalloc((void**) &blockDirections_device, noTotalEntries * sizeof(TSDFDirection)));

	summingVoxelMap = new SummingVoxelMap_CUDA;
}

void
ITMSceneReconstructionEngine_CUDA::IntegrateIntoSceneRayCasting(
	Scene* scene, const ITMView* view, const ITMTrackingState* trackingState,
	const ITMRenderState* renderState)
{
	ITMTimer timer;
	timer.Tick();

	Matrix4f invM_d = trackingState->pose_d->GetInvM();
	Vector4f projParams_d = view->calib.intrinsics_d.projectionParamsSimple.all;
	Vector4f projParams_rgb = view->calib.intrinsics_rgb.projectionParamsSimple.all;

	float* depth = view->depth->GetData(MEMORYDEVICE_CUDA);
	Vector4f* depthNormals = view->depthNormal->GetData(MEMORYDEVICE_CUDA);
	float* confidence = view->depthConfidence->GetData(MEMORYDEVICE_CUDA);
	Vector4u* rgb = view->rgb->GetData(MEMORYDEVICE_CUDA);
	ITMVoxel* localVBA = scene->localVBA.GetVoxelBlocks();
	ITMHashEntry* hashTable = scene->index.GetEntries();

	Vector2i depthImgSize = view->depth->noDims;

	Vector4f invProjParams_d = invertProjectionParams(projParams_d);
	auto* renderState_vh = (ITMRenderState_VH*) renderState;

	/// 1. Initialize summing voxel map (allocate and reset)
	summingVoxelMap->Init(hashTable, renderState_vh->GetVisibleEntryIDs(), renderState_vh->noVisibleEntries);

	/// 2. Ray cast update for every pixel
	dim3 blockSizeUpdate(16, 16);
	dim3 gridSizeUpdate((int) ceil((float) depthImgSize.x / (float) blockSizeUpdate.x),
	                    (int) ceil((float) depthImgSize.y / (float) blockSizeUpdate.y));
	rayCastUpdate_device << < gridSizeUpdate, blockSizeUpdate >> > (
		depthImgSize, view->rgb->noDims, depth, depthNormals, rgb, invM_d, trackingState->pose_d->GetM(),
		invProjParams_d, projParams_rgb, this->settings->fusionParams, this->settings->sceneParams,
		summingVoxelMap->getMap());
	ORcudaKernelCheck;
	this->timeStats.fusion += timer.Tock();

	/// 3. Ray cast space carving for every pixel
	timer.Tick();
	if (this->settings->fusionParams.useSpaceCarving)
	{
		if (this->settings->fusionParams.carvingMode == CarvingMode::CARVINGMODE_RAY_CASTING)
		{
			rayCastCarveSpace_device << < gridSizeUpdate, blockSizeUpdate >> > (
				depthImgSize, depth, depthNormals, invM_d, invProjParams_d, projParams_rgb, this->settings->fusionParams, this->settings->sceneParams,
					hashTable, summingVoxelMap->getMap(), localVBA);
			ORcudaKernelCheck;
		} else
		{
			Vector2i rgbImgSize = view->rgb->noDims;
			int* visibleEntryIDs = renderState_vh->GetVisibleEntryIDs();
			Matrix4f M_d = trackingState->pose_d->GetM();
			Matrix4f M_rgb = view->calib.trafo_rgb_to_depth.calib_inv * M_d;

			dim3 blockSizeVoxelProjection(SDF_BLOCK_SIZE, SDF_BLOCK_SIZE, SDF_BLOCK_SIZE);
			dim3 gridSizeVoxelProjection(renderState_vh->noVisibleEntries);
			voxelProjectionCarveSpace_device<false> << < gridSizeVoxelProjection, blockSizeVoxelProjection >> > (
				localVBA, summingVoxelMap->getVoxels(), hashTable, visibleEntryIDs,
					rgb, rgbImgSize, depth, depthNormals,
					confidence, depthImgSize, M_d, M_rgb, projParams_d,
					projParams_rgb, this->settings->fusionParams,
					this->settings->sceneParams);
			ORcudaKernelCheck;
		}
	}
	this->timeStats.carving = timer.Tock();

	/// 4. Collect per summation voxels, update actual voxel
	timer.Tick();
	dim3 blockSizeCombine(SDF_BLOCK_SIZE, SDF_BLOCK_SIZE, SDF_BLOCK_SIZE);
	dim3 gridSizeCombine(renderState_vh->noVisibleEntries);
	rayCastCombine_device << < gridSizeCombine, blockSizeCombine >> > (
		localVBA,
			hashTable,
			renderState_vh->GetVisibleEntryIDs(),
			summingVoxelMap->getVoxels(),
			this->settings->sceneParams
	);
	ORcudaKernelCheck;

	this->timeStats.fusion += timer.Tock();
}

ITMSceneReconstructionEngine_CUDA::~ITMSceneReconstructionEngine_CUDA(void)
{
	ORcudaSafeCall(cudaFreeHost(allocationTempData_host));
	ORcudaSafeCall(cudaFree(allocationTempData_device));
	ORcudaSafeCall(cudaFree(entriesAllocType_device));
	ORcudaSafeCall(cudaFree(blockCoords_device));
	ORcudaSafeCall(cudaFree(blockDirections_device));
	delete summingVoxelMap;
}

void ITMSceneReconstructionEngine_CUDA::ResetScene(Scene* scene)
{
	scene->tsdf->clear();
}

struct copyIndexPosition
{
	__host__ __device__ const ITMIndex &operator()(const ITMIndexDirectional &x) const {return ITMIndex(x.getPosition().toShort());}
};

void ITMSceneReconstructionEngine_CUDA::AllocateSceneFromDepth(Scene* scene,
                                                               const ITMView* view,
                                                               const ITMTrackingState* trackingState,
                                                               const ITMRenderState* renderState,
                                                               bool onlyUpdateVisibleList, bool resetVisibleList)
{
	ITMTimer timer;
	timer.Tick();
	Vector2i depthImgSize = view->depth->noDims;
	float voxelSize = scene->sceneParams->voxelSize;

	Matrix4f M_d, invM_d;
	Vector4f projParams_d, invProjParams_d;

	ITMRenderState_VH* renderState_vh = (ITMRenderState_VH*) renderState;

	if (resetVisibleList) renderState_vh->noVisibleEntries = 0;

	M_d = trackingState->pose_d->GetM();
	M_d.inv(invM_d);

	projParams_d = view->calib.intrinsics_d.projectionParamsSimple.all;
	invProjParams_d = invertProjectionParams(projParams_d);

	float mu = scene->sceneParams->mu;

	float* depth = view->depth->GetData(MEMORYDEVICE_CUDA);
	Vector4f* depthNormal = view->depthNormal->GetData(MEMORYDEVICE_CUDA);
	int* voxelAllocationList = scene->localVBA.GetAllocationList();
	int* excessAllocationList = scene->index.GetExcessAllocationList();
	ITMHashEntry* hashTable = scene->index.GetEntries();
	ITMHashSwapState* swapStates = scene->globalCache != NULL ? scene->globalCache->GetSwapStates(true) : 0;

	int noTotalEntries = scene->index.noTotalEntries;

	int* visibleEntryIDs = renderState_vh->GetVisibleEntryIDs();
	HashEntryVisibilityType* entriesVisibleType = renderState_vh->GetEntriesVisibleType();

	dim3 cudaBlockSizeHV(16, 16);
	dim3 gridSizeHV((int) ceil((float) depthImgSize.x / (float) cudaBlockSizeHV.x),
	                (int) ceil((float) depthImgSize.y / (float) cudaBlockSizeHV.y));

	dim3 cudaBlockSizeAL(256, 1);
	dim3 gridSizeAL((int) ceil((float) noTotalEntries / (float) cudaBlockSizeAL.x));

	dim3 cudaBlockSizeVS(256, 1);
	dim3 gridSizeVS((int) ceil((float) renderState_vh->noVisibleEntries / (float) cudaBlockSizeVS.x));

	AllocationTempData* tempData = (AllocationTempData*) allocationTempData_host;
	tempData->noAllocatedVoxelEntries = scene->localVBA.lastFreeBlockId;
	tempData->noAllocatedExcessEntries = scene->index.GetLastFreeExcessListId();
	tempData->noVisibleEntries = 0;
	memcpy(tempData->noAllocationsPerDirection, scene->localVBA.noAllocationsPerDirection,
	       sizeof(unsigned int) * N_DIRECTIONS);
	if (scene->tsdf->allocatedBlocks >= scene->tsdf->allocatedBlocksMax)
	{
		printf("No more free blocks. Allocation stopped.\n");
	}
	ORcudaSafeCall(
		cudaMemcpyAsync(allocationTempData_device, tempData, sizeof(AllocationTempData), cudaMemcpyHostToDevice));

	ORcudaSafeCall(cudaMemsetAsync(entriesAllocType_device, 0, sizeof(unsigned char) * noTotalEntries));

	if (gridSizeVS.x > 0)
	{
		setToType3 << < gridSizeVS, cudaBlockSizeVS >> >
		                            (entriesVisibleType, visibleEntryIDs, renderState_vh->noVisibleEntries);
		ORcudaKernelCheck;
	}

	{
		stdgpu::unordered_set<ITMIndexDirectional> allocationBlocks = stdgpu::unordered_set<ITMIndexDirectional>::createDeviceObject(1e6);
		findAllocationBlocks_device<<<gridSizeHV, cudaBlockSizeHV>>>(allocationBlocks, depth, depthNormal, invM_d,
		                                                             invProjParams_d, mu, depthImgSize, voxelSize,
		                                                             scene->sceneParams->viewFrustum_min,
		                                                             scene->sceneParams->viewFrustum_max,
		                                                             this->settings->fusionParams);

		size_t N = allocationBlocks.size();
		allocationBlocksList->Resize(N);
		thrust::copy(allocationBlocks.device_range().begin(), allocationBlocks.device_range().end(), stdgpu::device_begin(allocationBlocksList->GetData(MEMORYDEVICE_CUDA)));

		// Copy positions of allocation blocks (ignore direction) as list of visible blocks (approximation only!)
		renderState_vh->Resize(N);
		thrust::transform(allocationBlocks.device_range().begin(), allocationBlocks.device_range().end(), stdgpu::device_begin(renderState_vh->GetVisibleBlocks()), copyIndexPosition());

		auto tsdf = dynamic_cast<TSDF_CUDA<ITMIndexDirectional, ITMVoxel>*>(scene->tsdf);
		tsdf->allocate(allocationBlocksList->GetData(MEMORYDEVICE_CUDA), N);

		stdgpu::unordered_set<ITMIndexDirectional>::destroyDeviceObject(allocationBlocks);
	}
}

void ITMSceneReconstructionEngine_CUDA::IntegrateIntoSceneVoxelProjection(
	Scene* scene, const ITMView* view,
	const ITMTrackingState* trackingState, const ITMRenderState* renderState)
{
	ITMTimer timer;
	timer.Tick();

	Vector2i rgbImgSize = view->rgb->noDims;
	Vector2i depthImgSize = view->depth->noDims;
	float voxelSize = scene->sceneParams->voxelSize;

	Matrix4f M_d, M_rgb;
	Vector4f projParams_d, projParams_rgb;

	ITMRenderState_VH* renderState_vh = (ITMRenderState_VH*) renderState;
	if (renderState_vh->noVisibleEntries == 0) return;

	M_d = trackingState->pose_d->GetM();
	if (ITMVoxel::hasColorInformation) M_rgb = view->calib.trafo_rgb_to_depth.calib_inv * M_d;

	projParams_d = view->calib.intrinsics_d.projectionParamsSimple.all;
	projParams_rgb = view->calib.intrinsics_rgb.projectionParamsSimple.all;

	float* depth = view->depth->GetData(MEMORYDEVICE_CUDA);
	Vector4f* depthNormals = nullptr;
	if (this->settings->fusionParams.useWeighting or
	    this->settings->fusionParams.fusionMetric == FUSIONMETRIC_POINT_TO_PLANE)
		depthNormals = view->depthNormal->GetData(MEMORYDEVICE_CUDA);
	float* confidence = view->depthConfidence->GetData(MEMORYDEVICE_CUDA);
	Vector4u* rgb = view->rgb->GetData(MEMORYDEVICE_CUDA);
	ITMVoxel* localVBA = scene->localVBA.GetVoxelBlocks();
	ITMHashEntry* hashTable = scene->index.GetEntries();

	int* visibleEntryIDs = renderState_vh->GetVisibleEntryIDs();
	stdgpu::unordered_map<ITMIndexDirectional, ITMVoxel*> tsdf = ((TSDF_CUDA<ITMIndexDirectional, ITMVoxel>*)scene->tsdf)->getMap();

	dim3 cudaBlockSize(SDF_BLOCK_SIZE, SDF_BLOCK_SIZE, SDF_BLOCK_SIZE);
	dim3 gridSize(renderState_vh->noVisibleEntries);

	if (scene->sceneParams->stopIntegratingAtMaxW)
	{
		integrateIntoScene_device<true> << < gridSize, cudaBlockSize >> > (tsdf, allocationBlocksList->GetData(MEMORYDEVICE_CUDA),
			rgb, rgbImgSize, depth, depthNormals, confidence, depthImgSize, M_d, M_rgb, projParams_d, projParams_rgb, voxelSize,
			this->settings->fusionParams, this->settings->sceneParams);
		ORcudaKernelCheck;
	} else
	{
		integrateIntoScene_device<false> << < gridSize, cudaBlockSize >> > (tsdf, allocationBlocksList->GetData(MEMORYDEVICE_CUDA),
			rgb, rgbImgSize, depth, depthNormals, confidence, depthImgSize, M_d, M_rgb, projParams_d, projParams_rgb, voxelSize,
			this->settings->fusionParams, this->settings->sceneParams);
		ORcudaKernelCheck;
	}

	this->timeStats.fusion = timer.Tock();
}
